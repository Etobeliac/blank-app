import requests
from bs4 import BeautifulSoup
import time
import random
import pandas as pd

# URL de la page à scraper
url = 'https://member.expireddomains.net/domains/pendingdelete/?savedsearch_id=482709&ftlds[]=2&ftlds[]=3&ftlds[]=4&flimit=200&fdomainnot=kamagra+stromectol+pharmacy+levitra+cialis+outlet+viagra+sex+xanax+porn+nike+cheap+jersey+slot+casino&fenddays=1&fenddaysmax=2&fmseocf=8&fmseotf=8&fmseorefdomlive=21&fmseorefdomlivemax=99&o=adddate&r=d'

# Effectuer une requête GET à la page
response = requests.get(url)
response.raise_for_status()  # S'assurer que la requête a réussi

# Parser le contenu HTML
soup = BeautifulSoup(response.content, 'html.parser')

# Trouver toutes les cellules de la colonne "Domain"
domain_cells = soup.select('table.base1 > tbody > tr > td:nth-child(2)')

# Extraire le texte de chaque cellule avec des délais aléatoires
domains = []
for cell in domain_cells:
    domains.append(cell.text.strip())
    # Ajouter un délai aléatoire entre 5 et 20 secondes
    time.sleep(random.uniform(5, 20))

# Créer un DataFrame pandas
df = pd.DataFrame(domains, columns=["Domain"])

# Enregistrer les résultats dans un fichier Excel
output_path = 'domains.xlsx'
df.to_excel(output_path, index=False)

print(f"Les domaines ont été sauvegardés dans le fichier {output_path}")
